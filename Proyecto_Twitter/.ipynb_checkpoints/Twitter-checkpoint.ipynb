{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2: Clasificador de tweets\n",
    "## por Federico Pardo García\n",
    "\n",
    "## Máster en BIG DATA: Tecnologías de Gestión de Información No Estructurada 2020/2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "¿Empezamos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "SEED_VALUE = 1234\n",
    "\n",
    "api = twitter.Api(consumer_key='vJx3WkjUjATxveo8NK3tsVniM',\n",
    "                  consumer_secret='DLuw7WXYXwLIAVG2GtQHcVjOWM8Mq5a1qn3ZynniQSbSfKWSvM',\n",
    "                  access_token_key='769542991832907780-yM58uOw32rNLJe0OD3EBmD5KwdsScwv',\n",
    "                  access_token_secret='wBAtr8mhrOBQZE1ccv1y3tSAO8om2afmodatnj8gKLswn',\n",
    "                  sleep_on_rate_limit=True, # Nos permite despreocuparnos de rate_limits\n",
    "                  tweet_mode='extended')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(first_200, screen_name, last_id):\n",
    "    all_tweets = []\n",
    "    all_tweets.extend(first_200)\n",
    "    for i in range(5000):\n",
    "        new = api.GetUserTimeline(screen_name=screen_name, max_id = last_id-1)\n",
    "        if len(new) > 0:\n",
    "            all_tweets.extend(new)\n",
    "            last_id = new[-1].id\n",
    "        else:\n",
    "            break\n",
    "    return all_tweets\n",
    "\n",
    "def dump_XML(tweets, name):\n",
    "    items = ET.Element(\"data\")\n",
    "    for tweet in tweets:\n",
    "        element = ET.SubElement(items, 'tweet')\n",
    "        element.set('id', str(tweet.id))\n",
    "        element.set('user_id', str(tweet.user.id))\n",
    "        element.set('user_name', tweet.user.screen_name)\n",
    "        element.set('date', str(tweet.created_at_in_seconds))\n",
    "        element.text = tweet.full_text + '\\n'\n",
    "            \n",
    "\n",
    "    mydata = ET.ElementTree(items)\n",
    "    mydata.write(\"./extracted_users/\" + name + \".xml\")\n",
    "    \n",
    "    \n",
    "def extract_XML(filename):\n",
    "    tree = ET.ElementTree(file = \"./extracted_users/\" + filename)\n",
    "    root = tree.getroot()\n",
    "    collection = []\n",
    "    for doc in root:\n",
    "        collection.append(doc.text)\n",
    "    return collection\n",
    "    \n",
    "def timeline(name):\n",
    "    first200 = api.GetUserTimeline(screen_name=name, count=200)\n",
    "    all_tweets = get_tweets(first200, name, first200[-1].id)\n",
    "    dump_XML(all_tweets, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducimos los nombres de usuario\n",
    "users = ['GeorgeRussell63','LandoNorris', 'BillGates']\n",
    "for user in users:\n",
    "    if not os.path.exists(\"./extracted_users/\"+user+\".xml\"):\n",
    "        timeline(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de personalidades similares\n",
    "\n",
    "### Dividimos en 70% train y 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_TrainTest(vectorizedDS, corpus1_len, corpus2_len):\n",
    "    '''\n",
    "    Dado el vector de un corpus y las longitudes de los corpus que lo componen,\n",
    "    lo separará en train y test con sus correspondientes etiquetas.\n",
    "    '''\n",
    "    \n",
    "    # Separamos el data frame según el autor del tweet\n",
    "    corpus1vectorized = vectorizedDS[:corpus1_len]\n",
    "    corpus2vectorized = vectorizedDS[corpus2_len:]\n",
    "    \n",
    "    # Punto de corte para cada autor (70%)\n",
    "    nCorpus1 = int(corpus1_len*0.7)\n",
    "    nCorpus2 = int(corpus2_len*0.7)\n",
    "    \n",
    "    # Predictores de entrenamiento (usamos los más antiguos)\n",
    "    x1train = corpus1vectorized[corpus1_len-nCorpus1:]\n",
    "    x2train = corpus2vectorized[corpus2_len-nCorpus2:]\n",
    "    frames = [x1train,x2train]\n",
    "    xTrain = pd.concat(frames)\n",
    "\n",
    "    # Predictores de test (usamos los más modernos)\n",
    "    x1test = corpus1vectorized[:corpus1_len-nCorpus1]\n",
    "    x2test = corpus2vectorized[:corpus2_len-nCorpus2]\n",
    "    frames = [x1test,x2test]\n",
    "    xTest = pd.concat(frames)\n",
    "    \n",
    "    # Salida de entrenamiento\n",
    "    y1train = (np.zeros((nCorpus1, 1))).tolist()\n",
    "    y2train = (np.ones((nCorpus2, 1))).tolist()\n",
    "    y1train.extend(y2train)\n",
    "    yTrain = y1train\n",
    "    \n",
    "    # Salida de test\n",
    "    y1test = (np.zeros((corpus1_len-nCorpus1, 1))).tolist()\n",
    "    y2test = (np.ones((corpus2_len-nCorpus2, 1))).tolist()\n",
    "    y1test.extend(y2test)\n",
    "    yTest = y1test\n",
    "    \n",
    "    return (xTrain, yTrain, xTest, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bag_of_words(corpus):\n",
    "    '''\n",
    "    Devuelve una representación \"bag of words\" para un corpus de documentos.\n",
    "    '''\n",
    "    english_stop_words = text.ENGLISH_STOP_WORDS\n",
    "    vectorizer = CountVectorizer(stop_words = english_stop_words, min_df=20)\n",
    "    corpusVectorized = vectorizer.fit_transform(corpus)\n",
    "    df_tf = pd.DataFrame(corpusVectorized.todense(), columns=vectorizer.get_feature_names())\n",
    "    return df_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "russell = extract_XML('GeorgeRussell63.xml')\n",
    "norris = extract_XML('LandoNorris.xml')\n",
    "corpus = russell + norris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf = get_bag_of_words(corpus)\n",
    "(xTrain, yTrain, xTest, yTest) = split_TrainTest(df_tf, len(russell), len(norris))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento con SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "modelCV = GridSearchCV(clf,\n",
    "                       hyperparameters,\n",
    "                       cv=5,\n",
    "                       scoring='accuracy',\n",
    "                       return_train_score=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
